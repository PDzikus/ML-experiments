{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston housing prices prediction\n",
    "regression problem with Deep Learning/Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module keras.datasets.boston_housing in keras.datasets:\n",
      "\n",
      "NAME\n",
      "    keras.datasets.boston_housing - Boston housing price regression dataset.\n",
      "\n",
      "FUNCTIONS\n",
      "    load_data(path='boston_housing.npz', test_split=0.2, seed=113)\n",
      "        Loads the Boston Housing dataset.\n",
      "        \n",
      "        # Arguments\n",
      "            path: path where to cache the dataset locally\n",
      "                (relative to ~/.keras/datasets).\n",
      "            test_split: fraction of the data to reserve as test set.\n",
      "            seed: Random seed for shuffling the data\n",
      "                before computing the test split.\n",
      "        \n",
      "        # Returns\n",
      "            Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
      "\n",
      "DATA\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "\n",
      "FILE\n",
      "    /home/dzikus/anaconda3/envs/tensor/lib/python3.6/site-packages/keras/datasets/boston_housing.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import boston_housing\n",
    "help(boston_housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 2us/step\n",
      "Train data shape: (404, 13)\n",
      "Test data shape:  (102, 13)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape:  {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. building Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation = 'relu', input_shape = ( train_data.shape[1], )))\n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of small amount of data we'll be using for this model, we need to use K-fold cross-validation scheme for training to avoid (reduce) overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold #0\n",
      "processing fold #1\n",
      "processing fold #2\n",
      "processing fold #3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "    print(f\"processing fold #{i}\")\n",
    "    val_data = train_data[i*num_val_samples:(i+1)*num_val_samples]\n",
    "    val_targets = train_targets[i*num_val_samples:(i+1)*num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate([train_data[:i*num_val_samples], train_data[(i+1)*num_val_samples:]], axis = 0)\n",
    "    partial_train_targets = np.concatenate([train_targets[:i*num_val_samples], train_targets[(i+1)*num_val_samples:]], axis = 0)\n",
    "    \n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data, partial_train_targets, epochs = num_epochs, batch_size = 1, verbose = 0)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose = 0)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error per fold: [2.129848477864029, 2.250108553631471, 2.5153640437834333, 2.768015454311182]\n",
      "mean MSE: 2.415834132397529\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Square Error per fold: {all_scores}\")\n",
    "print(f\"mean MSE: {np.mean(all_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### new version of model training\n",
    "    This time I will train model with saved history of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold #0\n",
      "processing fold #1\n",
      "processing fold #2\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "    print(f\"processing fold #{i}\")\n",
    "    val_data = train_data[i*num_val_samples: (i+1)*num_val_samples]\n",
    "    val_targets = train_targets[i*num_val_samples: (i+1)*num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate([train_data[:i*num_val_samples], train_data[(i+1)*num_val_samples:]], axis = 0)\n",
    "    partial_train_targets = np.concatenate([train_targets[:i*num_val_samples], train_targets[(i+1)*num_val_samples:]], axis = 0)\n",
    "    \n",
    "    model = build_model()\n",
    "    history = model.fit(partial_train_data, partial_train_targets, validation_data = (val_data, val_targets),\n",
    "                        epochs = num_epochs, batch_size = 1, verbose = 0)\n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "    all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range (num_epochs)]\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validataion MAE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because above plot is hard to read, we'll remove first 10 data points, and the rest of the plot will be smoothed - ech point will be replaced with an exponential moving average of the previous points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor = 0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "smooth_mae_history = smooth_curve(average_mae_history[10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model starts to overfit around 80th epoch, so we'll build a fresh new model with only 80 epochs and all train_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "mode.fit(train_data, train_targets, epochs = 80, batch_size = 1, verbose = 0)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n",
    "print(f\"Final model MAE score: {test_mae_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.6 (Tensor)",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
